{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "* export SPARK_HOME=...\n",
    "* export PYSPARK_DRIVER_PYTHON=jupyter\n",
    "* export PYSPARK_DRIVER_PYTHON_OPTS='notebook'\n",
    "* pyspark --packages org.apache.bahir:spark-sql-cloudant_2.11:2.2.0,com.ibm.stocator:stocator:1.0.17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor ObjectStorage(s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_endpoint=\"YOUR_ENDPOINT\"\n",
    "s3_access_key=\"YOUR_ACCESS_KEY\"\n",
    "s3_secret_key=\"YOUR_SECRET_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hconf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hconf.set('fs.cos.impl', 'com.ibm.stocator.fs.ObjectStoreFileSystem')\n",
    "hconf.set('fs.stocator.scheme.list', 'cos')\n",
    "hconf.set('fs.stocator.cos.impl', 'com.ibm.stocator.fs.cos.COSAPIClient')\n",
    "hconf.set('fs.stocator.cos.scheme', 'cos')\n",
    "hconf.set('fs.cos.mycos.endpoint', s3_endpoint)\n",
    "hconf.set('fs.cos.mycos.access.key', s3_access_key)\n",
    "hconf.set('fs.cos.mycos.secret.key', s3_secret_key)\n",
    "hconf.set('fs.cos.mycos.v2.signer.type', 'false')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cloudant_host = \"YOUR_HOST\"\n",
    "cloudant_username = \"YOUR_USER\"\n",
    "cloudant_password = \"YOUR_PASSWORD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict\n",
    "\n",
    "def loadData (fileName):\n",
    "    \n",
    "    rdd=sc.textFile(fileName)\n",
    "    temp = rdd.map(lambda k: k.split(\"=\",1))\n",
    "    fields = [StructField(\"name\", StringType(), True),StructField(\"value\", StringType(), True)]\n",
    "    schema = StructType(fields)   \n",
    "    df = sqlContext.createDataFrame(temp, schema) \n",
    "    df.show()\n",
    "    return df\n",
    "\n",
    "def collectData (rdd, names):\n",
    "    \n",
    "    dict = rdd.filter(lambda x: x['name'] in names).collectAsMap()\n",
    "    dict = {str(k): str(v) for k, v in dict.items()}\n",
    "    return dict  \n",
    "\n",
    "def process(dir):\n",
    "    \n",
    "    print \"process \", dir\n",
    "    \n",
    "    print \"... load data\"\n",
    "    df = sqlContext.read\\\n",
    "      .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "      .option('header', 'false')\\\n",
    "      .option('mode', 'DROPMALFORMED')\\\n",
    "      .option('inferSchema', 'true')\\\n",
    "      .load(dir+\"transactions.dat\")\n",
    "    df.show()\n",
    "    \n",
    "    key = (concat(col(\"_c0\"), lit(\"-\"), col(\"_c1\")))\n",
    "    df_convert = df.withColumn('name', key).withColumn('value',col(\"_c2\")).select(\"name\",\"value\") \n",
    "    \n",
    "    df_env = loadData(dir+\"environment.dat\")\n",
    "    df_load = loadData(dir+\"workload.dat\")\n",
    "    \n",
    "    print \"... filter data\"\n",
    "\n",
    "    col_load = collectData(df_load.rdd,[\"db.driver\",\"recordcount\",\"operationcount\"])\n",
    "    col_env = collectData(df_env.rdd,[\"YCSB_THREAD_COUNT\",\"YCSB_WORKLOAD\",\"YCSB_OP\",\"YCSB_DB\",\"YCSB_NOTES\",\"YCSB_RUN_DATE\"])\n",
    "    col_perf = collectData(df_convert.rdd, [\"[OVERALL]- RunTime(ms)\",\"[OVERALL]- Throughput(ops/sec)\", \\\n",
    "                                        \"[READ]- Operations\", \\\n",
    "                                        \"[READ]- AverageLatency(us)\", \\\n",
    "                                        \"[READ]- MinLatency(us)\", \\\n",
    "                                        \"[READ]- MaxLatency(us)\", \\\n",
    "                                        \"[READ]- 95thPercentileLatency(us)\", \\\n",
    "                                        \"[READ]- 99thPercentileLatency(us)\", \\\n",
    "                                        \"[READ]- Return=OK\", \\\n",
    "                                        \"[UPDATE]- Operations\", \\\n",
    "                                        \"[UPDATE]- AverageLatency(us)\", \\\n",
    "                                        \"[UPDATE]- MinLatency(us)\", \\\n",
    "                                        \"[UPDATE]- MaxLatency(us)\", \\\n",
    "                                        \"[UPDATE]- 95thPercentileLatency(us)\", \\\n",
    "                                        \"[UPDATE]- 99thPercentileLatency(us)\", \\\n",
    "                                        \"[UPDATE]- Return=OK\" ])\n",
    "\n",
    "    print \"... aggregate data\"\n",
    "\n",
    "    col_all = dict(col_perf, **col_load)\n",
    "    col_all = dict(col_all, **col_env)\n",
    "\n",
    "    df_all = sc.parallelize([col_all]).map(lambda l: Row(**dict(l))).toDF()\n",
    "    \n",
    "    print \"...transform data\"\n",
    "\n",
    "    df_converted = df_all.withColumn(\"YCSB_THREAD_COUNT\", df_all[\"YCSB_THREAD_COUNT\"].cast(IntegerType())) \\\n",
    "               .withColumn(\"YCSB_RUN_DATE\", from_unixtime(unix_timestamp('YCSB_RUN_DATE', 'yyyy-MM-dd_HH:mm:ss')).cast(TimestampType())) \\\n",
    "               .withColumn(\"operationcount\", df_all[\"operationcount\"].cast(IntegerType())) \\\n",
    "               .withColumn(\"recordcount\", df_all[\"recordcount\"].cast(IntegerType())) \\\n",
    "               .withColumn(\"[OVERALL]- RunTime(ms)\", df_all[\"[OVERALL]- RunTime(ms)\"].cast(FloatType())).withColumnRenamed(\"[OVERALL]- RunTime(ms)\", \"Latency\") \\\n",
    "               .withColumn(\"[OVERALL]- Throughput(ops/sec)\", df_all[\"[OVERALL]- Throughput(ops/sec)\"].cast(FloatType())).withColumnRenamed(\"[OVERALL]- Throughput(ops/sec)\", \"Throughput\") \\\n",
    "               .withColumn(\"[READ]- 95thPercentileLatency(us)\", df_all[\"[READ]- 95thPercentileLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[READ]- 95thPercentileLatency(us)\", \"Read_95th\") \\\n",
    "               .withColumn(\"[READ]- 99thPercentileLatency(us)\", df_all[\"[READ]- 99thPercentileLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[READ]- 99thPercentileLatency(us)\", \"Read_99th\")  \\\n",
    "               .withColumn(\"[READ]- AverageLatency(us)\", df_all[\"[READ]- AverageLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[READ]- AverageLatency(us)\", \"Read_Latency\") \\\n",
    "               .withColumn(\"[READ]- MaxLatency(us)\", df_all[\"[READ]- MaxLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[READ]- MaxLatency(us)\", \"Read_Max_Latency\") \\\n",
    "               .withColumn(\"[READ]- MinLatency(us)\", df_all[\"[READ]- MinLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[READ]- MinLatency(us)\", \"Read_Min_Latency\") \\\n",
    "               .withColumn(\"[READ]- Operations\", df_all[\"[READ]- Operations\"].cast(IntegerType())).withColumnRenamed(\"[READ]- Operations\", \"Read_Operations\") \\\n",
    "               .withColumn(\"[READ]- Return=OK\", df_all[\"[READ]- Return=OK\"].cast(IntegerType())).withColumnRenamed(\"[READ]- Return=OK\", \"Read_Operations_Success\") \\\n",
    "               .withColumn(\"[UPDATE]- 95thPercentileLatency(us)\", df_all[\"[UPDATE]- 95thPercentileLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[UPDATE]- 95thPercentileLatency(us)\", \"Update_95th\") \\\n",
    "               .withColumn(\"[UPDATE]- 99thPercentileLatency(us)\", df_all[\"[UPDATE]- 99thPercentileLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[UPDATE]- 99thPercentileLatency(us)\", \"Update_99th\") \\\n",
    "               .withColumn(\"[UPDATE]- AverageLatency(us)\", df_all[\"[UPDATE]- AverageLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[UPDATE]- AverageLatency(us)\", \"Update_Latency\") \\\n",
    "               .withColumn(\"[UPDATE]- MaxLatency(us)\", df_all[\"[UPDATE]- MaxLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[UPDATE]- MaxLatency(us)\", \"Update_Max_Latency\") \\\n",
    "               .withColumn(\"[UPDATE]- MinLatency(us)\", df_all[\"[UPDATE]- MinLatency(us)\"].cast(FloatType())).withColumnRenamed(\"[UPDATE]- MinLatency(us)\", \"Update_Min_Latency\") \\\n",
    "               .withColumn(\"[UPDATE]- Operations\", df_all[\"[UPDATE]- Operations\"].cast(IntegerType())).withColumnRenamed(\"[UPDATE]- Operations\", \"Update_Operations\") \\\n",
    "               .withColumn(\"[UPDATE]- Return=OK\", df_all[\"[UPDATE]- Return=OK\"].cast(IntegerType())).withColumnRenamed(\"[UPDATE]- Return=OK\", \"Update_Operations_Success\") \n",
    "   \n",
    "    df_converted.printSchema()\n",
    "    df_converted.show()\n",
    "    \n",
    "    print \"... save data\"\n",
    "    df_converted.write.format(\"org.apache.bahir.cloudant\") \\\n",
    "            .option(\"cloudant.host\",cloudant_host) \\\n",
    "            .option(\"cloudant.username\",cloudant_username) \\\n",
    "            .option(\"cloudant.password\",cloudant_password) \\\n",
    "            .save(\"ycsb\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirName = \"cos://mycompose.mycos/mysql/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileName(data):\n",
    "    if data.isEmpty():\n",
    "        return \"\"\n",
    "    debug = data.toDebugString()\n",
    "    lines = debug.split(\"\\n\")[2:] \n",
    "    for l in lines: \n",
    "        fullName=l.split()[1]\n",
    "        fName = fullName.split(\"/\")[-1]\n",
    "        fDir = fullName[0:len(fullName)-len(fName)]\n",
    "        print fDir\n",
    "        print fName\n",
    "        if (fName == \"workload.dat\"):\n",
    "            process(fDir)\n",
    "\n",
    "ssc = StreamingContext(sc, 1)\n",
    "files = ssc.textFileStream(dirName)\n",
    "files.foreachRDD(fileName)\n",
    "print(files)\n",
    "ssc.start()\n",
    "ssc.awaitTermination()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
